\section{Dirbtiniai neuroniniai tinklai}

        Dirbtiniai neuroniniai tinklai (angl.~\textit{Artificial Neural Networks}) - mašininio mokymosi sritis, apimanti skaitinius modelius, pasižyminčius sluoksnine architektūra (angl.~\textit{layered architecture}) bei mokymusi, siekiant aproksimuoti matematinį sąryšį tarp duomenų įvesties ir numatytos išvesties \cite{gfg_ann_overview}. Kadangi dirbtiniai neuroniniai tinklai taip pat pasižymi netiesiškumu ir sugebėjimu apibendrinti duomenis, šie modeliai yra tinkami naudoti užduotyse, reikalaujančiose sudėtingų šablonų (angl.~\textit{pattern}) atpažinimo. Šios užduotys yra skirstomos į dvi rūšis: klasifikavimą ir regresiją.
        
        Šio kursinio projektinio darbo metu buvo pasirinkta spręsti regresijos problemą, siekiant įvertinti dirbtinių neuroninių tinklų apmokymo sąlygas, kai mokymosi duomenys nėra visiškai tikslūs ir turi triukšmo.

        \paragraph{Klasifikavimas}

            Klasifikavimas - užduotis, kurioje modelis iš numatytos diskrečios aibės turi kiekvienam duomenų įrašui priskirti vieną ar kelias reikšmes. Pavyzdžiui, klasifikavimo uždavinio pavyzdžiu galėtų būti klasių \enquote{katė} arba \enquote{šuo} priskyrimas pateiktoms nuotraukoms. Kitas pavyzdys galėtų būti augalo rūšies nustatymas pagal duotus konkretaus objekto parametrus: žiedo spalvą, lapų matmenis ir pan. 
        
        \paragraph{Regresija}
        
            Regresijos uždavinius sprendžiantys modeliai yra skirti tolydžiųjų kintamųjų reikšmių prognozavimui. Tokio tipo uždaviniams priskiriami, pavyzdžiui, finansų rinkų statistinių rodiklių prognozavimas, temperatūros nustatymas remiantis istoriniais duomenimis bei kiti panašaus pobūdžio taikymai.
    
    \subsection{Sandara}

        Dirbtiniai neuroniniai tinklai yra sudaryti iš tarpusavyje sujungtų neuronų, sudarančių tinklą. Žemiau pateiktas \ref{img:dnt} paveikslėlis iliustruoja dirbtinių neuroninių tinklų sandarą.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\linewidth]{attachments/img/dnt.jpg}
            \caption{Dirbtinių neuroninių tinklų sandara.}
            \label{img:dnt}
        \end{figure}

        \subsubsection{Neuronai}

            Neuronas dirbtiniuose neuroniniuose tinkluose yra pagrindinė sudedamoji - tai apdorojimo vienetas (angl.~\textit{processing unit}), priimantis įvestį iš vienų neuronų ir generuojantis išvestį kitiems. Neuronai yra atsakingi už skaičiavimus, kurie padeda modeliui išgryninti sąryšį tarp duomenų įvesties ir numatomos išvesties. Šiai užduočiai atlikti neuronai pasitelkia svertinės sumos ir aktyvavimo funkcijas. Žemiau pateiktas \ref{img:dnt_neuronas} paveikslėlis iliustruoja dirbtinių neuroninių tinklų neurono sandarą.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.5\linewidth]{attachments/img/dnt_neuronas.jpg}
                \caption{Neurono sudedamosios dirbtiniuose neuroniniuose tinkluose.}
                \label{img:dnt_neuronas}
            \end{figure}

            \subsubsubsection{Svertinės sumos funkcija}

                Kaip jau buvo minėta neuronai tarpusavyje yra sujungti ir taip sudaro tinklą. Kiekviena jungtis tarp neuronų yra apibūdinama svoriu - skaitine reikšme, nusakančia, kokio dydžio įtaką vienas neuronas turi kitam. Šios reikšmės yra naudojamos svertinės sumos funkcijos reikšmei apskaičiuoti, kurios pavidalas yra
                \begin{equation}
                    \label{eq:svertines_sumos_funkc}
                    z = \sum_{i=0}^{n} x_i \cdot w_i + b
                \end{equation}
                Čia $z$ - svertinės sumos reikšmė, $x_i$ - i-ojo neurono įvestis nagrinėjama neuronui, $w_i$ - i-ojo neurono svoris, susijęs su nagrinėjamu neuronu, $b$ - šališkumo (angl.~\textit{bias}) reikšmė.

                Ši funkcija yra reikalinga tam, kad neuronas galėtų apibendrinti visas gautas įvestis į vieną skaitinę vertę, kuri vėliau perduodama aktyvavimo funkcijai, jei ji yra naudojama, arba kitam neuronų sluoksniui.

            \subsubsubsection{Aktyvavimo funkcijos}

                Dirbtiniai neuroniniai tinklai taikomi ne tik su tiesiniais, bet ir su netiesines savybes turinčiais duomenim. Jei modeliai naudotų vien svertinės sumos funkcijos apskaičiavimus, jie generuotų tik tiesines išvestis ir nebūtų tinkami atpažinti sudėtingesnius duomenų šablonus \cite{gfg_activation_funcs}. Siekiant įveikti šį apribojimą, neuroniniuose tinkluose yra naudojamos aktyvavimo funkcijos.

                Aktyvavimo funkcijos pasirinkimas priklauso nuo sprendžiamo uždavinio specifikos. Žemiau pateiktos \ref{eq:aktyvavimo_funkc_sigmoidine}, \ref{eq:aktyvavimo_funkc_hiperbolinio_tangento}, \ref{eq:aktyvavimo_funkc_relu} funkcijos yra dažniausiai neuroniniuose tinkluose naudojamos aktyvavimo funkcijos \cite{rasamoelina_activation_funcs}.

                Sigmoidinė (angl.~\textit{Sigmoid}) funkcija:
                \begin{equation}
                    \label{eq:aktyvavimo_funkc_sigmoidine}
                    \sigma(x) = \frac{1}{1 + e^{-x}}
                \end{equation}

                Hiperbolinio tangento (angl.~\textit{Hyperbolic Tangent}) funkcija:
                \begin{equation}
                    \label{eq:aktyvavimo_funkc_hiperbolinio_tangento}
                    f(x) = \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
                \end{equation}

                Rektifikuoto tiesinio elemento (angl.~\textit{Rectified Linear Unit}) funkcija:
                \begin{equation}
                    \label{eq:aktyvavimo_funkc_relu}
                    \text{ReLU}(x) =
                    \begin{cases}
                        0, \text{ jei $x \leq 0$ } \\ 
                        x, \text{ jei $x > 0$ }
                    \end{cases}
                \end{equation}

        \subsubsection{Sluoksniai dirbtiniuose neuroniniuose tinkluose}

            Dirbtiniai neuroniniai tinklai išsiskiria nuo kitų mašininio mokymosi metodų savo architektūra - neuronų tinklas yra sudarytas sluoksniais. Dažniausiai dirbtinius neuroninius tinklus sudaro vienas įvesties, vienas ar keli paslėptieji ir vienas išvesties sluoksnis. Žemiau pateiktas \ref{img:dnt_sluoksniai} pav. iliustruoja dirbtinių neuroninių tinklų sandarą atsižvelgiant į neuronų sluoksnius.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.5\linewidth]{attachments/img/dnt_sluoksniai.jpg}
                \caption{Dirbtinių neuroninių tinklų sluoksniai.}
                \label{img:dnt_sluoksniai}
            \end{figure}

            \paragraph{Įvesties sluoksnis}

                Įvesties sluoksnis dirbtiniuose neuroniniuose tinkluose yra atsakingas už duomenų surinkimą. Neuronų skaičius šiame sluoksnyje priklauso nuo to, kokios dimensijos duomenys yra nagrinėjami, neįskaitant klasės ar spėjamos reikšmės. Taip, pavyzdžiui, turint duomenis su dviem požymiais, įvesties sluoksnyje bus du neuronai. Tuo tarpu turint šešių dimensijų duomenis, neuronų įvesties sluoksnyje bus šeši.

            \paragraph{Išvesties sluoksnis}
                
                Išvesties sluoksnis dirbtiniuose neuroniniuose tinkluose yra naudojamas modelio rezultatams generuoti. Neuronų kiekis šiame sluoksnyje priklauso nuo sprendžiamos užduoties tipo. Klasifikavimo uždaviniuose išvesties sluoksnio neuronų skaičius paprastai sutampa su galimų klasių skaičiumi, o kiekvieno neurono išvestis interpretuojama kaip tikimybė, kad įvesties duomenys priklauso tai klasei. Tuo tarpu regresijos uždaviniuose dažniausiai naudojamas vienas neuronas, kurio išvestis atitinka prognozuojamą reikšmę.
            
            \paragraph{Paslėptieji sluoksniai}
                
                Paslėptieji sluoksniai dirbtiniuose neuroniniuose tinkluose yra skirti matematinių ryšių tarp įvesties ir išvesties nustatymui. Šiuose sluoksniuose esančių neuronų kiekis yra hiperparametras - jis yra parenkamas eksperimentiškai, analizuojant, su kuria reikšme modelis pasirodo geriausiai validacijos metu.

    \subsection{Dirbtinio neuroninio tinklo apmokymas su triukšmu}

        Kalbant apie triukšmo naudą dirbtinių neuroninių tinklų apmokymo kontekste, Zur ir kiti lygino triukšmo įterpimą su populiariausiomis reguliavimo technikomis, tokiomis kaip svorių mažinimas (angl.~\textit{weight decay}) ir išankstinis stabdymas (angl.~\textit{early stopping}) \cite{zur_noise_comparison}. Šis straipsnis parodė, kad triukšmo pridėjimas mokymo metu gali sumažinti persimokymą, kai kuriais atvejais pasiekiant geresnius rezultatus nei ankstyvas stabdymas, o jo poveikis modelio gebėjimui apibendrinti duomenis yra lyginamas su įprastais reguliavimo metodais. Tai patvirtina, kad triukšmo naudojimas gali būti laikomas efektyvia alternatyva arba papildomu reguliavimo įrankiu, ypač tais atvejais, kai klasikiniai metodai nepakankamai apsaugo modelį nuo persimokymo.

        Kitas straipsnis pateikė teorinį paaiškinimą, kad triukšmo įterpimas į dirbtinio neuroninio tinklo neuronų aktyvavimą matematiškai atitinka tam tikrą baudą stipriai varijuojančiom funkcijos dalim, lemdamas modelių gebėjimą apibendrinti lygesnes funkcijas \cite{camuto_understanding_gaussian_noise}. Šis straipsnis parodė, kad triukšmo įterpimas mažina dirbtinio neuroninio tinklo jautrumą duomenų variacijoms ir skatinanti stabilumą bei nuoseklumą prognozėse.

        Dar vienas straipsnis parodė, kad triukšmo įterpimas kartu su papildomomis baudos funkcijomis didina modelio stabilumą, išlaikant priimtiną prognozių tikslumą \cite{nguyen_training_more_robust_classification}. Šis straipsnis parodo, kad triukšmas dirbtinių neuroninių tinklų apmokymo kontekste leidžia išvengti persimokymo, o papildomos baudos leidžia kontroliuoti modelio atsparumą triukšmui, užtikrinant tikslumą.
        
        Apibendrinant, šie darbai patvirtina, kad triukšmo įterpimas yra veiksminga strategija modelio persimokymo mažinimui ir atsparumo didinimui. Tačiau jo efektyvumas priklauso nuo kelių veiksnių, įskaitant tinklo architektūrą, triukšmo intensyvumą, taikomas baudos funkcijas bei konkrečią uždavinio specifiką. Be to, tyrimai rodo, kad optimalūs rezultatai pasiekiami derinant triukšmo įterpimą su kitomis reguliavimo priemonėmis, leidžiančiomis užtikrinti tiek stabilumą, tiek aukštą prognozavimo tikslumą.