{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1fecf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.data_generation import generate_vectors, generate_scalars\n",
    "from benchmark_functions.sphere import sphere_func\n",
    "from neural_network_models.feedforward_nn import FeedforwardNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac78adc",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d533119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0024174850>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "torch.manual_seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f55bafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimention = 5\n",
    "domain = [-5, 5]\n",
    "\n",
    "data_set_size = 10_000\n",
    "training_set_fraction = 0.7\n",
    "validation_set_fraction = 0.15\n",
    "test_set_fraction = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a72e4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "data_set_vectors = generate_vectors(input_dimention, domain, data_set_size)\n",
    "data_set_scalars = generate_scalars(data_set_vectors, sphere_func)\n",
    "\n",
    "# separate training data from validation and test data\n",
    "training_set_vectors, temp_set_vectors, training_set_scalars, temp_set_scalars = train_test_split(\n",
    "    data_set_vectors, \n",
    "    data_set_scalars, \n",
    "    test_size = (validation_set_fraction + test_set_fraction), \n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "# separate validation data from test data\n",
    "validation_set_vectors, test_set_vectors, validation_set_scalars, test_set_scalars = train_test_split(\n",
    "    temp_set_vectors, \n",
    "    temp_set_scalars, \n",
    "    test_size = (test_set_fraction / (test_set_fraction + validation_set_fraction)), \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff4d59",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "752c47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_scaler = MinMaxScaler().fit(training_set_vectors)\n",
    "scalar_scaler = StandardScaler().fit(training_set_scalars.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eba6a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_vectors = vector_scaler.transform(training_set_vectors)\n",
    "validation_set_vectors = vector_scaler.transform(validation_set_vectors)\n",
    "test_set_vectors = vector_scaler.transform(test_set_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b9e20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scalars = scalar_scaler.transform(training_set_scalars.reshape(-1, 1))\n",
    "validation_set_scalars = scalar_scaler.transform(validation_set_scalars.reshape(-1, 1))\n",
    "test_set_scalars = scalar_scaler.transform(test_set_scalars.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1133dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert generated data to tensors\n",
    "training_set_vectors = torch.FloatTensor(training_set_vectors)\n",
    "training_set_scalars = torch.FloatTensor(training_set_scalars)\n",
    "\n",
    "validation_set_vectors = torch.FloatTensor(validation_set_vectors)\n",
    "validation_set_scalars = torch.FloatTensor(validation_set_scalars)\n",
    "\n",
    "test_set_vectors = torch.FloatTensor(test_set_vectors)\n",
    "test_set_scalars = torch.FloatTensor(test_set_scalars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95b99f",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "236e2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNN(input_neuron_num = input_dimention, h1_neuron_num = 10, output_neuron_num = 1)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "loss_optimization_func = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/500:5] Validation loss: 1.027023, Training loss: 0.999570\n",
      "[200/500:5] Validation loss: 1.027304, Training loss: 0.998785\n",
      "[300/500:5] Validation loss: 1.025284, Training loss: 0.997097\n",
      "[400/500:5] Validation loss: 1.008454, Training loss: 0.985433\n",
      "[500/500:5] Validation loss: 0.949289, Training loss: 0.935712\n"
     ]
    }
   ],
   "source": [
    "learning_epochs = 500\n",
    "\n",
    "for epoch in range(learning_epochs):\n",
    "    # pass forward\n",
    "    training_set_predictions = model(training_set_vectors)\n",
    "    training_loss = loss_func(training_set_predictions, training_set_scalars)\n",
    "\n",
    "    # back-propagation\n",
    "    loss_optimization_func.zero_grad()\n",
    "    training_loss.backward()\n",
    "    loss_optimization_func.step()\n",
    "\n",
    "    # validation step\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        validation_set_predictions = model(validation_set_vectors)\n",
    "        validation_loss = loss_func(validation_set_predictions, validation_set_scalars)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"[{epoch + 1}/{learning_epochs}] Validation loss: {validation_loss.item():.6f}, Training loss: {training_loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8198a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss (MSE): 0.966644\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_set_predictions = model(test_set_vectors)\n",
    "    test_loss = loss_func(test_set_predictions, test_set_scalars)\n",
    "    print(f\"Test loss (MSE): {test_loss.item():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
